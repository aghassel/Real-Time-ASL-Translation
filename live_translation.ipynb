{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Webcam to ASL Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - combination of a 1D CNN and a Transformer, and used 4x seed ensemble for submission\n",
    "From: https://github.com/hoyso48/Google---Isolated-Sign-Language-Recognition-1st-place-solution\n",
    "\n",
    "Latency: 17ms\n",
    "\n",
    "Accuracy: 89.29%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a 5 second buffer to mimic the length of data in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "ROWS_PER_FRAME = 543  # number of holistic landmarks\n",
    "data_columns = 3  # 'x', 'y', 'z' for each landmark\n",
    "BUFFER_SIZE = 5  # number of frames to buffer before making a prediction\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(static_image_mode=False, \n",
    "                                min_detection_confidence=0.5, \n",
    "                                min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "def load_label_map(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        label_map = json.load(file)\n",
    "    index_to_label = {v: k for k, v in label_map.items()}\n",
    "    return index_to_label\n",
    "\n",
    "index_to_label = load_label_map(\"sign_to_prediction_index_map.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(results):\n",
    "    landmarks = {'face': results.face_landmarks, 'left_hand': results.left_hand_landmarks,\n",
    "                 'pose': results.pose_landmarks, 'right_hand': results.right_hand_landmarks}\n",
    "    all_landmarks = []\n",
    "    for key, result in landmarks.items():\n",
    "        num_landmarks = {'face': 468, 'left_hand': 21, 'pose': 33, 'right_hand': 21}[key]\n",
    "        if result is None:\n",
    "            all_landmarks.extend([(0, 0, 0)] * num_landmarks)\n",
    "        else:\n",
    "            all_landmarks.extend([(landmark.x, landmark.y, landmark.z) for landmark in result.landmark])\n",
    "    return all_landmarks\n",
    "\n",
    "def update_buffer(landmarks_buffer, new_landmarks, buffer_size):\n",
    "    landmarks_buffer.append(new_landmarks)\n",
    "    if len(landmarks_buffer) > buffer_size:\n",
    "        landmarks_buffer.pop(0)\n",
    "    \n",
    "    return landmarks_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_buffer = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n",
    "\n",
    "    landmarks = extract_landmarks(results)\n",
    "    landmarks_buffer = update_buffer(landmarks_buffer, landmarks, BUFFER_SIZE)\n",
    "    predicted_label = None\n",
    "    labels = None\n",
    "    if len(landmarks_buffer) == BUFFER_SIZE:\n",
    "        flat_list = [item for sublist in landmarks_buffer for item in sublist]\n",
    "        df = pd.DataFrame(flat_list, columns = ['x', 'y', 'z'])\n",
    "        n_frames = int(len(df) / ROWS_PER_FRAME)\n",
    "        df = df.values.reshape(n_frames, ROWS_PER_FRAME, len(df.columns))\n",
    "        df = df.astype(np.float32)\n",
    "        prediction_fcn = interpreter.get_signature_runner('serving_default')\n",
    "        output = prediction_fcn(inputs=df)\n",
    "        p = output['outputs'].reshape(-1)\n",
    "        predicted_index = np.argmax(p)\n",
    "        if p[predicted_index] > 5:\n",
    "            predicted_label = index_to_label[predicted_index]\n",
    "        else:\n",
    "            predicted_label = None\n",
    "\n",
    "    cv2.putText(frame, f'Predicted: {predicted_label}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Confidence: {p[predicted_index]:.2f}', (10, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('ASL Word Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELEC475",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
